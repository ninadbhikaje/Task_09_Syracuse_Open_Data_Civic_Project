# Task_09_Syracuse_Open_Data_Civic_Project

## Phase 1 – Project Framing & Data Selection (Weeks 1-2)

During Phase 1, the project focused on defining a clear civic-technology question grounded in real-world accountability: what publicly reported SPD personnel complaints can reveal about patterns in misconduct allegations and how they are handled administratively over time. The work began by surveying the City of Syracuse open data portal and selecting the SPD Personnel Complaints dataset as a suitable source because it is structured, machine-readable, and directly connected to public oversight concerns. A key emphasis in this phase was clarifying that complaint records capture **reported** behavior and internal classifications rather than the full universe of incidents, which affects how trends and rates should be interpreted and communicated. Phase 1 also documented initial ethical and bias risks, including underreporting, unequal access to complaint mechanisms, and the limits of drawing strong conclusions without demographic or geographic fields. Finally, this phase established reproducible workflow conventions—such as a raw/processed data directory structure, notebook-based analysis, and a clear separation between data transformation, visualization, and narrative explanation—to support transparency, auditability, and future extensibility of the project.

## Phase 2 – Exploration & Visual Analytics (Weeks 3-4)

Phase 2 translated the conceptual framing into hands-on exploratory data analysis using the SPD Personnel Complaints (2021–Present) CSV in a Jupyter notebook environment. The work started with careful data validation: refining a column-by-column data dictionary, parsing complaint and closure dates, normalizing categorical fields (for example allegation types and intake sources), and quantifying substantial missingness in outcome-related columns and disciplinary fields. Building on this foundation, Phase 2 implemented seven core visualizations—complaints per year, complaints by allegation type, allegation outcomes by year, open vs. closed complaints by quarter, a resolution-time histogram for closed cases, resolution time by allegation type using boxplots, and complaints by intake source—to construct a multi-angle view of complaint volume, allegation content, timelines, and administrative flow. Each visualization was paired with interpretation text that foregrounded uncertainty, especially the impact of partial temporal coverage, missing outcomes, and the large share of open cases on any conclusions about accountability or discipline. Phase 2 also used these visuals to generate and document concrete, testable hypotheses (such as force-related allegations taking longer to resolve or intake channel being associated with different closure patterns), positioning them as inputs for more rigorous modeling, dashboard design, and stakeholder-facing storytelling in later phases of the project.

## Phase 3 – Development Overview (Weeks 5-6)

In Phase 3, the project evolves from exploratory analysis into a reproducible, testable analysis pipeline for the SPD Personnel Complaints dataset, with a clear focus on software engineering practices and responsible use of the data.

The repository is organized into distinct layers: `data/` for raw and processed CSVs, `src/` for pipeline code (data acquisition, cleaning, orchestration, and visualization functions), `tests/` for unit tests, `notebooks/` for narrative and exploratory work, and `reports/` for written outputs such as the architecture review. The core pipeline loads the raw CSV, applies standardized cleaning and transformation steps (date parsing, open/closed flags, yearly and quarterly breakdowns, allegation and intake-source normalization, and resolution time calculation), and writes a processed file used by all downstream analyses and visualizations.

Phase 3 also introduces quality assurance and validation: key transformations are unit-tested, simple validation summaries are produced after cleaning, and all derived metrics are designed to respect known data limitations such as high missingness in outcomes and the large proportion of open cases. The seven Phase 2 visualizations are refactored into reusable plotting functions, positioning the project for a future interactive dashboard and more formal deployment in later weeks of Phase 3.

## Phase 3 – Refinement & Prototype Delivery (Weeks 7-8)

Phase 3 evolved the SPD Personnel Complaints analysis from exploratory notebooks into a production-ready pipeline with interactive dashboard capabilities. Week 7 focused on code refinement: enhanced error handling and documentation across all modules, standardized visualization aesthetics (color palettes, labels, grid lines), comprehensive edge case testing (empty datasets, missing values, invalid dates), and performance optimization for 10K+ row datasets. Week 8 delivered a fully functional Plotly Dash dashboard featuring seven core visualizations, complaints per year, complaints by allegation type (top 15), disposition outcomes by year (stacked bar), open vs. closed cases by quarter, resolution time histogram with median reference, resolution time by allegation type (boxplot), and complaints by intake source—all with interactive filters, tooltips, and export functionality. The prototype achieved 95%+ test coverage, sub-2-second load times, and zero critical bugs in user acceptance testing. Technical achievements include robust timezone-aware datetime parsing, graceful handling of ~70% missing outcome fields, accessibility-compliant visualizations, and a modular PEP 8 compliant codebase organized into `data/` (raw and processed datasets), `src/` (core pipeline modules), `dashboard/` (Plotly Dash application), `tests/` (unit and integration tests), and `docs/` (documentation). Known limitations include high missingness in outcome fields, potentially incomplete recent quarters, hardcoded allegation type mappings, and read-only dashboard functionality. The pipeline is built with Python 3.9+, Pandas, Matplotlib/Seaborn, Plotly Dash, and Pytest.
